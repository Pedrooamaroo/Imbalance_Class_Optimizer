{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca4991cf",
   "metadata": {},
   "source": [
    "# **Improving Classification Algorithm for Difficult Data Conditions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55bf06c",
   "metadata": {},
   "source": [
    "In this assignment, we will be implementing a classification algorithm from scratch, analyze how a specific data issue affects its performance, propose a modification to improve it, and validate the results through empirical evaluation on benchmark datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc0f06b",
   "metadata": {},
   "source": [
    "# 1st Step - Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191ec064",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy matplotlib seaborn imbalanced-learn autograd tqdm nbformat plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0bf6cb",
   "metadata": {},
   "source": [
    "# 2nd Step - Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0baab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import base64\n",
    "import io\n",
    "import shutil\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "import logging\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319f1bbb",
   "metadata": {},
   "source": [
    "# 3rd Step - Algorithm Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5712cc5b",
   "metadata": {},
   "source": [
    "For this assignment we selected collectively to explore the Logistic Regression as our classification algorithm. We are going to use the standard version of the algorithm found on https://github.com/rushter/MLAlgorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597e4c78",
   "metadata": {},
   "source": [
    "Logistic Regression models the probability of class membership by applying the sigmoid function to a linear combination of input features, enabling it to separate classes based on learned decision boundaries. It is sensitive to both noise and class imbalance, with a particular vulnerability to class imbalance, as it naturally biases toward the majority class during training. Although extensions allow logistic regression to handle multiclass problems, its performance is primarily affected by noise and imbalance rather than the multiclass setting itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67652d4",
   "metadata": {},
   "source": [
    "For this specific reason we are going to evaluate our algorithm on the Dataset Group 2: Class imbalance in binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59153280",
   "metadata": {},
   "source": [
    "# 4th Step - Preprocess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f477b3",
   "metadata": {},
   "source": [
    "For our algorithm to work as expected we need to make sure our data is clean and ready to be used, so we need to check the following cases:\n",
    "\n",
    " - Files categorical values on it;\n",
    " - Files with missing values.\n",
    "\n",
    "After we look up these files and fix them we also need to make sure that all our \"target\" classes have the same name, and after that we are going to attribute 1 to the minority classes and 0 to the majorities ones, to easier future comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b393f2",
   "metadata": {},
   "source": [
    "Let´s start by seeing how files have categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a922631d",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"class_imbalance\"\n",
    "\n",
    "total_files = 0\n",
    "files_with_categorical = 0\n",
    "files_with_categorical_list = []\n",
    "\n",
    "for file in os.listdir(folder):\n",
    "    if file.endswith(\".csv\"):\n",
    "        try:\n",
    "            df = pd.read_csv(os.path.join(folder, file))\n",
    "            total_files += 1\n",
    "\n",
    "            print(f\"\\nFile: {file}\")\n",
    "            print(f\"   Shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "            # Check for categorical features\n",
    "            feature_cols = df.columns[:-1]  # exclude target\n",
    "            categorical_features = df[feature_cols].select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "            if categorical_features:\n",
    "                print(f\"   Categorical features detected: {categorical_features}\")\n",
    "                files_with_categorical += 1\n",
    "                files_with_categorical_list.append(file)\n",
    "            else:\n",
    "                print(\"   No categorical features detected.\")\n",
    "\n",
    "            # Get the last column as the target\n",
    "            target_col = df.columns[-1]\n",
    "            print(f\"   Target column: '{target_col}'\")\n",
    "\n",
    "            # Print class distribution\n",
    "            print(\"\\nClass distribution:\")\n",
    "            print(df[target_col].value_counts(normalize=True))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with file {file}: {e}\")\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n\\nTotal files processed: {total_files}\")\n",
    "print(f\"Files with categorical features: {files_with_categorical}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41201b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFiles with categorical features:\")\n",
    "for f in files_with_categorical_list:\n",
    "    print(f\"- {f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d4051",
   "metadata": {},
   "source": [
    "16 files with categorical values as expected!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eb609d",
   "metadata": {},
   "source": [
    "Now let's check for missing values, we are going to use common representations of missing values to garantee no mistake is made and to not let missing values go unseen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f48b1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"class_imbalance\"\n",
    "\n",
    "# Variable to track how many files have missing values\n",
    "files_with_missing_values = 0\n",
    "\n",
    "# Define common representations of missing values\n",
    "common_na = [\"\", \"NA\", \"NaN\", \"n/a\", \"N/A\", \"null\", \"NULL\", \"-\", \"--\", \"?\", \"none\", \"None\", \" \"]\n",
    "\n",
    "# Loop through all CSV files in the folder\n",
    "for file in os.listdir(folder):\n",
    "    if file.endswith(\".csv\"):\n",
    "        try:\n",
    "            file_path = os.path.join(folder, file)\n",
    "\n",
    "            # Read CSV with common missing value representations\n",
    "            df = pd.read_csv(file_path, na_values=common_na)\n",
    "\n",
    "            print(f\"\\nFile: {file}\")\n",
    "\n",
    "            # Check for missing values\n",
    "            missing_values = df.isnull().sum()\n",
    "            if missing_values.sum() == 0:\n",
    "                print(\"No missing values.\")\n",
    "            else:\n",
    "                print(\"Missing values:\")\n",
    "                print(missing_values[missing_values > 0])\n",
    "                files_with_missing_values += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with file {file}: {e}\")\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n\\nTotal files with missing values: {files_with_missing_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10cff62",
   "metadata": {},
   "source": [
    "Now that we know we have 16 files with categorical features and 10 with missing values we are going to fix them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0fbf76",
   "metadata": {},
   "source": [
    "After a quick analysis we can see that some files have entire features with missing values, so we are going to remove them since they are useless for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97adf479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output folders\n",
    "input_folder = \"class_imbalance\"\n",
    "cleaned_folder = \"class_imbalance_cleaned\"\n",
    "imputed_folder = \"class_imbalance_imputed\"\n",
    "\n",
    "# Create output folders if they don't exist\n",
    "os.makedirs(cleaned_folder, exist_ok=True)\n",
    "os.makedirs(imputed_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af20bd8",
   "metadata": {},
   "source": [
    "Once our new folders are created we can move on to cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbae2dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_files_cleaned = 0\n",
    "\n",
    "for file in os.listdir(input_folder):\n",
    "    if file.endswith(\".csv\"):\n",
    "        input_path = os.path.join(input_folder, file)\n",
    "        output_path = os.path.join(cleaned_folder, file)\n",
    "\n",
    "        try:\n",
    "            # Read CSV\n",
    "            df = pd.read_csv(input_path)\n",
    "            initial_columns = df.columns.tolist()\n",
    "\n",
    "            # Drop columns that are all NaN\n",
    "            df_cleaned = df.dropna(axis=1, how='all')\n",
    "            final_columns = df_cleaned.columns.tolist()\n",
    "\n",
    "            # Detect removed columns\n",
    "            removed_columns = list(set(initial_columns) - set(final_columns))\n",
    "\n",
    "            # Save cleaned file\n",
    "            df_cleaned.to_csv(output_path, index=False)\n",
    "            total_files_cleaned += 1\n",
    "\n",
    "            # Only print if some columns were removed\n",
    "            if removed_columns:\n",
    "                print(f\"\\nFile: {file}\")\n",
    "                print(f\"Removed columns: {removed_columns}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error cleaning {file}: {e}\")\n",
    "\n",
    "print(f\"\\n\\nTotal files cleaned: {total_files_cleaned}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f23c77",
   "metadata": {},
   "source": [
    "We can proceed to impute the missing values, we are doing so by using the mode for binary and categorical features and using the KNN for the remaining!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8192b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_binary(series):\n",
    "    \"\"\"Checks if a series is binary (ignoring NaNs).\"\"\"\n",
    "    unique_vals = series.dropna().astype(str).str.lower().unique()\n",
    "    return len(unique_vals) == 2\n",
    "\n",
    "def is_numeric(series):\n",
    "    \"\"\"Checks if a series is numeric.\"\"\"\n",
    "    try:\n",
    "        pd.to_numeric(series.dropna())\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def apply_smart_imputer(df):\n",
    "    \"\"\"Applies smart imputation: mode for binary/categorical, KNN for numeric.\"\"\"\n",
    "    feature_cols = df.columns[:-1]  # Last column is target\n",
    "    target_col = df.columns[-1]\n",
    "    \n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[target_col]\n",
    "\n",
    "    binary_cols = []\n",
    "    categorical_cols = []\n",
    "    numeric_cols = []\n",
    "\n",
    "    for col in X.columns:\n",
    "        if is_binary(X[col]):\n",
    "            binary_cols.append(col)\n",
    "        elif not is_numeric(X[col]) or X[col].nunique() <= 10:\n",
    "            categorical_cols.append(col)\n",
    "        else:\n",
    "            numeric_cols.append(col)\n",
    "\n",
    "    print(f\"Imputing {len(binary_cols)} binary, {len(categorical_cols)} categorical, {len(numeric_cols)} numeric columns.\")\n",
    "\n",
    "    # Binary + Categorical: fill with mode\n",
    "    for col in binary_cols + categorical_cols:\n",
    "        if not X[col].mode().empty:\n",
    "            mode_val = X[col].mode()[0]\n",
    "            X[col] = X[col].fillna(mode_val)\n",
    "        else:\n",
    "            print(f\"Warning: Column '{col}' has no mode (all NaNs). Skipping.\")\n",
    "\n",
    "    # Numeric: KNN impute\n",
    "    valid_numeric = X[numeric_cols].dropna(axis=1, how='all')\n",
    "    removed = set(numeric_cols) - set(valid_numeric.columns)\n",
    "\n",
    "    if not valid_numeric.empty:\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "        imputed_array = imputer.fit_transform(valid_numeric)\n",
    "        X[valid_numeric.columns] = imputed_array\n",
    "\n",
    "    if removed:\n",
    "        print(f\"Skipped all-NaN numeric columns: {removed}\")\n",
    "\n",
    "    # Return combined DataFrame with target\n",
    "    return pd.concat([X, y], axis=1)\n",
    "\n",
    "def validate_imputation(original_df, imputed_df):\n",
    "    \"\"\"Compare missing values before and after imputation.\"\"\"\n",
    "    feature_cols = original_df.columns[:-1]\n",
    "\n",
    "    before_missing = original_df[feature_cols].isnull().sum()\n",
    "    after_missing = imputed_df[feature_cols].isnull().sum()\n",
    "\n",
    "    still_missing = after_missing[after_missing > 0]\n",
    "\n",
    "    if still_missing.empty:\n",
    "        print(\"✅ Imputation successful: no missing values remain.\")\n",
    "    else:\n",
    "        print(\"⚠️ Some columns still have missing values:\")\n",
    "        print(still_missing)\n",
    "\n",
    "    # Optional: show change summary\n",
    "    total_before = before_missing.sum()\n",
    "    total_after = after_missing.sum()\n",
    "    print(f\"Missing before: {total_before} → after: {total_after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983d2509",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process cleaned files with smart imputer\n",
    "\n",
    "total_files_imputed = 0\n",
    "\n",
    "for file in os.listdir(cleaned_folder):\n",
    "    if file.endswith(\".csv\"):\n",
    "        input_path = os.path.join(cleaned_folder, file)\n",
    "        output_path = os.path.join(imputed_folder, file)\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(input_path)\n",
    "            df_imputed = apply_smart_imputer(df)\n",
    "\n",
    "            df_imputed.to_csv(output_path, index=False)\n",
    "            total_files_imputed += 1\n",
    "\n",
    "            print(f\"Imputed and saved: {file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error imputing {file}: {e}\")\n",
    "\n",
    "print(f\"\\nTotal files imputed: {total_files_imputed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f696b52",
   "metadata": {},
   "source": [
    "Let´s see if our code ran correctly checking again for missing values and manually see if the imputation was done correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b78f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable to track how many files have missing values\n",
    "files_with_missing_values = 0\n",
    "\n",
    "# Loop through all CSV files in the folder\n",
    "for file in os.listdir(imputed_folder):\n",
    "    if file.endswith(\".csv\"):\n",
    "        try:\n",
    "            df = pd.read_csv(os.path.join(imputed_folder, file))\n",
    "\n",
    "            print(f\"\\nFile: {file}\")\n",
    "\n",
    "            # Check for missing values\n",
    "            missing_values = df.isnull().sum()\n",
    "            if missing_values.sum() == 0:\n",
    "                print(\"No missing values.\")\n",
    "            else:\n",
    "                print(\"Missing values:\")\n",
    "                print(missing_values[missing_values > 0])\n",
    "                files_with_missing_values += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with file {file}: {e}\")\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n\\nTotal files with missing values: {files_with_missing_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed545ad",
   "metadata": {},
   "source": [
    "Now that we have none missing values we can use Label Encoder and One-Hot Encoder to verify that all our data is numerical. For simple categories like \"Yes/No\", we use Label Encoding (assigning 0 and 1). For multiple categories like colors, we apply One-Hot Encoding, creating separate columns with 1s and 0s to avoid false numerical relationships.\n",
    "\n",
    "We'll handle the target column separately later, ensuring the minority class is labeled as 1 and majority as 0 for clarity. These transformations give us clean, numerical data ready for machine learning models to process effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12eb17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_features(df, ohe_threshold):\n",
    "    feature_cols = df.columns[:-1]  # Para não tocar na coluna target\n",
    "    target_col = df.columns[-1]\n",
    "\n",
    "    categorical_features = df[feature_cols].select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    if not categorical_features:\n",
    "        return df, [], []\n",
    "\n",
    "    df_encoded = df.copy()\n",
    "    ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    ohe_cols = []\n",
    "    label_cols = []\n",
    "\n",
    "    for col in categorical_features:\n",
    "        unique_values = df_encoded[col].nunique()\n",
    "\n",
    "        if unique_values <= ohe_threshold:\n",
    "            # One-Hot Encoder\n",
    "            ohe_df = pd.DataFrame(\n",
    "                ohe.fit_transform(df_encoded[[col]]),\n",
    "                columns=[f\"{col}{cat}\" for cat in ohe.categories_[0]],\n",
    "                index=df_encoded.index\n",
    "            )\n",
    "            df_encoded = pd.concat([df_encoded.drop(columns=[col]), ohe_df], axis=1)\n",
    "            print(f\" One-Hot Encoded '{col}' ({unique_values} categorias)\")\n",
    "            ohe_cols.append(col)\n",
    "        else:\n",
    "            # Label Encoder\n",
    "            df_encoded[col] = pd.factorize(df_encoded[col])[0]\n",
    "            print(f\" Label Encoded '{col}' ({unique_values} categorias)\")\n",
    "            label_cols.append(col)\n",
    "\n",
    "    # Manter target no fim\n",
    "    cols = [col for col in df_encoded.columns if col != target_col] + [target_col]\n",
    "    df_encoded = df_encoded[cols]\n",
    "\n",
    "    return df_encoded, ohe_cols, label_cols\n",
    "\n",
    "def process_files(original_folder, modified_folder, ohe_threshold):\n",
    "    os.makedirs(modified_folder, exist_ok=True)\n",
    "    report_lines = []\n",
    "    \n",
    "    datasets_with_encoders = 0  # Contador para datasets que utilizaram o encoder\n",
    "\n",
    "    for file in os.listdir(original_folder):\n",
    "        if file.endswith(\".csv\"):\n",
    "            original_path = os.path.join(original_folder, file)\n",
    "            modified_path = os.path.join(modified_folder, file)\n",
    "\n",
    "            try:\n",
    "                df = pd.read_csv(original_path)\n",
    "                df_encoded, ohe_cols, label_cols = encode_features(df, ohe_threshold)\n",
    "\n",
    "                df_encoded.to_csv(modified_path, index=False)\n",
    "                print(f\"Processed and saved: {file}\\n\")\n",
    "\n",
    "                # Verificar se o dataset utilizou algum encoder\n",
    "                if ohe_cols or label_cols:\n",
    "                    datasets_with_encoders += 1  # Incrementar se houver uso de encoder\n",
    "                \n",
    "                report_lines.append(f\"File: {file}\")\n",
    "                if ohe_cols:\n",
    "                    report_lines.append(f\"  One-Hot Encoded columns: {', '.join(ohe_cols)}\")\n",
    "                if label_cols:\n",
    "                    report_lines.append(f\"  Label Encoded columns: {', '.join(label_cols)}\")\n",
    "                if not ohe_cols and not label_cols:\n",
    "                    report_lines.append(\"  No categorical columns detected.\")\n",
    "                report_lines.append(\"\")  # linha em branco entre ficheiros\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {e}\\n\")\n",
    "                report_lines.append(f\"Error processing {file}: {e}\")\n",
    "                report_lines.append(\"\")\n",
    "\n",
    "    # Guardar relatório\n",
    "    report_path = os.path.join(modified_folder, \"encoding_report.txt\")\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(report_lines))\n",
    "\n",
    "    print(\"\\nAll files processed! Report saved to 'encoding_report.txt'\")\n",
    "\n",
    "    # Imprimir o número de datasets que utilizaram o encoder\n",
    "    print(f\"\\nTotal datasets processed with encoders (OHE or Label Encoding): {datasets_with_encoders}\")\n",
    "    return datasets_with_encoders\n",
    "\n",
    "\n",
    "# Parâmetros de execução\n",
    "original_folder = \"class_imbalance_imputed\"\n",
    "modified_folder = \"class_imbalance_modified\"\n",
    "ohe_threshold = 15\n",
    "\n",
    "# Chamada da função para processar os arquivos\n",
    "datasets_with_encoders = process_files(original_folder, modified_folder, ohe_threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fdfbad",
   "metadata": {},
   "source": [
    "We can see that the total datasets processed with encoders were 16, the same number of datasets with categorical values in them! Also, we have the report saved in the folder to check manually if everything went as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaef14d",
   "metadata": {},
   "source": [
    "Now that we have a folder with all the data cleaned we just need to do the last step, name the last column of all datasets as \"target\" and make sure the minority class is 1 and 0 the majority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d5c7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output folders\n",
    "input_folder = \"class_imbalance_modified\"             \n",
    "output_folder = \"class_imbalance_final\"      \n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Process each CSV file in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if not filename.lower().endswith(\".csv\"):\n",
    "        continue  # skip non-CSV files\n",
    "    \n",
    "    # Read the dataset\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Rename the last column to 'target'\n",
    "    df.rename(columns={df.columns[-1]: \"target\"}, inplace=True)\n",
    "    \n",
    "    # Only handle binary classification cases\n",
    "    counts = df['target'].value_counts()\n",
    "    if len(counts) != 2:\n",
    "        # Skip files that are not binary\n",
    "        print(f\"Skipping {filename}: not binary classification.\")\n",
    "        continue\n",
    "    \n",
    "    # Identify majority and minority class labels\n",
    "    minority_label = counts.idxmin()  # label with fewer instances\n",
    "    majority_label = counts.idxmax()  # label with more instances\n",
    "    \n",
    "    # Map the minority class to 1 and the majority class to 0\n",
    "    df['target'] = df['target'].apply(lambda x: 1 if x == minority_label else 0)\n",
    "    \n",
    "    # Save the modified dataset to the new folder\n",
    "    output_path = os.path.join(output_folder, filename)\n",
    "    df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502ddaeb",
   "metadata": {},
   "source": [
    "Let's check if everything is right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816d2c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"class_imbalance_final\"\n",
    "\n",
    "total_files = 0\n",
    "files_with_categorical = []\n",
    "files_with_missing = []\n",
    "\n",
    "for file in os.listdir(folder):\n",
    "    if not file.lower().endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(os.path.join(folder, file))\n",
    "    total_files += 1\n",
    "\n",
    "    print(f\"\\nFile: {file}\")\n",
    "    print(f\"   Shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "    # Record files that have any categorical features\n",
    "    feature_cols = df.columns[:-1]\n",
    "    if df[feature_cols].select_dtypes(include=['object', 'category']).any().any():\n",
    "        files_with_categorical.append(file)\n",
    "\n",
    "    # Record files that have any missing values\n",
    "    if df.isnull().any().any():\n",
    "        files_with_missing.append(file)\n",
    "\n",
    "    # Show target info\n",
    "    target_col = df.columns[-1]\n",
    "    print(f\"   Target column: '{target_col}'\")\n",
    "    print(\"   Class distribution:\")\n",
    "    print(df[target_col].value_counts(normalize=True))\n",
    "\n",
    "# Final summary counts\n",
    "print(f\"\\nFiles with categorical features: {len(files_with_categorical)}\")\n",
    "print(f\"Files with missing values: {len(files_with_missing)}\")\n",
    "print(f\"Total files processed: {total_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45308d8",
   "metadata": {},
   "source": [
    "Now we are reading to start our real work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d0abc1",
   "metadata": {},
   "source": [
    "# 5th Step - Model Training & Evaluation: Pre‑Changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1d9bb1",
   "metadata": {},
   "source": [
    "Now that we have our data ready we can start the model training and evaluation on our base model! Firstly let's define our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe72136",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1000)\n",
    "\n",
    "\n",
    "def binary_crossentropy(y_true, y_pred):\n",
    "    eps = 1e-8  # to avoid log(0)\n",
    "    return -np.mean(y_true * np.log(y_pred + eps) + (1 - y_true) * np.log(1 - y_pred + eps))\n",
    "\n",
    "\n",
    "class BasicRegression:\n",
    "    def __init__(self, lr=0.001, penalty=None, C=0.01, tolerance=1e-4, max_iters=1000):\n",
    "        self.lr = lr\n",
    "        self.penalty = penalty\n",
    "        self.C = C\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iters = max_iters\n",
    "        self.errors = []\n",
    "        self.theta = None\n",
    "        self.n_samples, self.n_features = None, None\n",
    "        self.cost_func = None\n",
    "\n",
    "    def _add_penalty(self, loss, w):\n",
    "        if self.penalty == \"l1\":\n",
    "            loss += self.C * np.abs(w[1:]).sum()\n",
    "        elif self.penalty == \"l2\":\n",
    "            loss += 0.5 * self.C * (w[1:] ** 2).sum()\n",
    "        return loss\n",
    "\n",
    "    def _cost(self, X, y, theta):\n",
    "        prediction = X.dot(theta)\n",
    "        error = self.cost_func(y, prediction)\n",
    "        return error\n",
    "\n",
    "    def _add_intercept(self, X):\n",
    "        b = np.ones([X.shape[0], 1])\n",
    "        return np.concatenate([b, X], axis=1)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y).reshape(-1)\n",
    "\n",
    "        self.X = self._add_intercept(X)\n",
    "        self.y = y\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "\n",
    "        self.init_cost()\n",
    "\n",
    "        self.theta = np.random.normal(size=(self.n_features + 1), scale=0.5)\n",
    "        self.theta, self.errors = self._gradient_descent()\n",
    "\n",
    "        logging.info(f\"Training completed. Final loss: {self.errors[-1]}\")\n",
    "\n",
    "    def _gradient_descent(self):\n",
    "        theta = self.theta\n",
    "        errors = [self._cost(self.X, self.y, theta)]\n",
    "        cost_d = grad(self._loss)\n",
    "\n",
    "        for i in range(1, self.max_iters + 1):\n",
    "            delta = cost_d(theta)\n",
    "            theta -= self.lr * delta\n",
    "\n",
    "            current_error = self._cost(self.X, self.y, theta)\n",
    "            errors.append(current_error)\n",
    "\n",
    "            logging.info(f\"Iteration {i}, error {current_error}\")\n",
    "\n",
    "            if np.abs(errors[-2] - errors[-1]) < self.tolerance:\n",
    "                logging.info(\"Convergence has reached.\")\n",
    "                break\n",
    "\n",
    "        return theta, errors\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        probs = self.predict_proba(X)\n",
    "        return (probs >= threshold).astype(int)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        raise NotImplementedError(\"This method should be implemented in a subclass.\")\n",
    "\n",
    "    def _loss(self, w):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def init_cost(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class LogisticRegression(BasicRegression):\n",
    "    def init_cost(self):\n",
    "        self.cost_func = binary_crossentropy\n",
    "\n",
    "    def _loss(self, w):\n",
    "        predictions = self.sigmoid(np.dot(self.X, w))\n",
    "\n",
    "        # Clamp predictions to avoid log(0)\n",
    "        eps = 1e-8\n",
    "        predictions = np.clip(predictions, eps, 1 - eps)\n",
    "\n",
    "        loss = self.cost_func(self.y, predictions)\n",
    "        return self._add_penalty(loss, w)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 0.5 * (np.tanh(0.5 * x) + 1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.array(X)\n",
    "        X = self._add_intercept(X)\n",
    "        return self.sigmoid(np.dot(X, self.theta))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a73122",
   "metadata": {},
   "source": [
    "We now are going to train our model and evaluate it but for that we need to standardize features, we are going to do that using StandardScaler. To garantee a fair training we are also going to make sure both classes appear in both training and testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e105e371",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data and Initialize Structures\n",
    "\n",
    "data_folder = \"class_imbalance_final\"\n",
    "csv_files = sorted(glob.glob(os.path.join(data_folder, \"*.csv\")))\n",
    "\n",
    "# Lists to accumulate results\n",
    "results = []  # Will hold dicts of metrics for each dataset\n",
    "roc_images = []  # Will hold (dataset_name, base64_png, auc) for plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e408638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process Each Dataset\n",
    "# For each CSV:\n",
    "# 1. Read data into X (features) and y (binary target).\n",
    "# 2. Repeatedly split (stratified) until both classes present in train and test.\n",
    "# 3. Scale features with StandardScaler (fit on train, transform both).\n",
    "# 4. Train logistic regression.\n",
    "# 5. Compute metrics on test set.\n",
    "# 6. Plot and save ROC curve for this dataset.\n",
    "\n",
    "for file_path in tqdm(csv_files, desc=\"Datasets\"):\n",
    "    dataset_name = os.path.basename(file_path)\n",
    "    df = pd.read_csv(file_path)\n",
    "    if 'target' not in df.columns:\n",
    "        # If not labeled, rename last col to 'target'\n",
    "        df = df.rename(columns={df.columns[-1]: 'target'})\n",
    "    X = df.drop(columns=['target']).values\n",
    "    y = df['target'].values\n",
    "\n",
    "    # Stratified split with retries to ensure both classes in train and test\n",
    "    seed = 0\n",
    "    while True:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.3, stratify=y, random_state=seed\n",
    "        )\n",
    "        # Check class presence\n",
    "        if len(np.unique(y_train)) == 2 and len(np.unique(y_test)) == 2:\n",
    "            break\n",
    "        seed += 1\n",
    "\n",
    "    # Feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Model training\n",
    "    model = LogisticRegression()  # using default parameters\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predictions and probabilities\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    try:\n",
    "        y_proba = model.predict_proba(X_test_scaled)[:, 1]  # probability of class 1\n",
    "    except:\n",
    "        # Some implementations might use a different method name\n",
    "        y_proba = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Compute metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Dataset': dataset_name,\n",
    "        'Accuracy': acc,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1-Score': f1,\n",
    "        'ROC AUC': auc\n",
    "    })\n",
    "\n",
    "    # Plot ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {auc:.3f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "    plt.title(f'ROC Curve: {dataset_name}')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot to base64\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    plt.close()\n",
    "    buf.seek(0)\n",
    "    img_base64 = base64.b64encode(buf.read()).decode('utf-8')\n",
    "    roc_images.append((dataset_name, img_base64, auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ff30bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile Metrics and Compute Averages\n",
    "\n",
    "metrics_df = pd.DataFrame(results)\n",
    "# Round metrics for display\n",
    "metrics_df[['Accuracy','Precision','Recall','F1-Score','ROC AUC']] = metrics_df[\n",
    "    ['Accuracy','Precision','Recall','F1-Score','ROC AUC']].round(3)\n",
    "\n",
    "# Compute average metrics\n",
    "avg_metrics = metrics_df[['Accuracy','Precision','Recall','F1-Score','ROC AUC']].mean().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc869326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Começar estrutura HTML\n",
    "html_lines = []\n",
    "\n",
    "html_lines.append(\"<html><head><title>Relatório Comparativo</title>\")\n",
    "html_lines.append(\"<style>\")\n",
    "html_lines.append(\"body {font-family: Arial, sans-serif; margin: 20px; background-color: #f0f8ff; color: #222;}\")\n",
    "html_lines.append(\"h1, h2, h3 { color: #007acc; }\")\n",
    "html_lines.append(\"table {border-collapse: collapse; width: 100%; margin-bottom: 30px; background-color: #ffffff;}\")\n",
    "html_lines.append(\"th, td {border: 1px solid #ddd; padding: 8px; text-align: center;}\")\n",
    "html_lines.append(\"th {background-color: #e6f2ff; cursor: pointer;}\")\n",
    "html_lines.append(\"th:hover {background-color: #d0e7ff;}\")\n",
    "html_lines.append(\"tr:nth-child(even) {background-color: #f9fbfd;}\")\n",
    "html_lines.append(\"tr:hover {background-color: #eef7ff;}\")\n",
    "html_lines.append(\"img {border: 1px solid #ccc; margin: 5px;}\")\n",
    "html_lines.append(\"a { color: #007acc; text-decoration: none; }\")\n",
    "html_lines.append(\"a:hover { text-decoration: underline; }\")\n",
    "html_lines.append(\"button { background-color: #007acc; color: white; border: none; padding: 8px 12px; border-radius: 4px; cursor: pointer; }\")\n",
    "html_lines.append(\"button:hover { background-color: #005f99; }\")\n",
    "html_lines.append(\"</style>\")\n",
    "html_lines.append(\"\"\"\n",
    "<script>\n",
    "function sortTable(n, id) {\n",
    "  var table = document.getElementById(id), rows, switching = true, i, x, y, shouldSwitch, dir = \"asc\", switchcount = 0;\n",
    "  while (switching) {\n",
    "    switching = false;\n",
    "    rows = table.rows;\n",
    "    for (i = 1; i < (rows.length - 1); i++) {\n",
    "      shouldSwitch = false;\n",
    "      x = rows[i].getElementsByTagName(\"TD\")[n];\n",
    "      y = rows[i + 1].getElementsByTagName(\"TD\")[n];\n",
    "      var xVal = isNaN(x.innerHTML) ? x.innerHTML.toLowerCase() : parseFloat(x.innerHTML);\n",
    "      var yVal = isNaN(y.innerHTML) ? y.innerHTML.toLowerCase() : parseFloat(y.innerHTML);\n",
    "      if ((dir === \"asc\" && xVal > yVal) || (dir === \"desc\" && xVal < yVal)) {\n",
    "        shouldSwitch = true; break;\n",
    "      }\n",
    "    }\n",
    "    if (shouldSwitch) {\n",
    "      rows[i].parentNode.insertBefore(rows[i + 1], rows[i]);\n",
    "      switching = true; switchcount++;\n",
    "    } else {\n",
    "      if (switchcount === 0 && dir === \"asc\") {\n",
    "        dir = \"desc\"; switching = true;\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "</script>\n",
    "\"\"\")\n",
    "html_lines.append(\"</head><body>\")\n",
    "html_lines.append('<a id=\"inicio\"></a>')\n",
    "html_lines.append(\"<h1>Relatório Comparativo: Modelos de Classificação</h1>\")\n",
    "html_lines.append('<div style=\"margin-bottom: 20px; font-size: 16px;\">')\n",
    "html_lines.append('<b>Ir para:</b> ')\n",
    "html_lines.append('<a href=\"#modelo1\">Baseline</a> | ')\n",
    "html_lines.append('<a href=\"#modelo2\">After Changes</a> | ')\n",
    "html_lines.append('<a href=\"#modelo3\">With SMOTE</a>')\n",
    "html_lines.append('<a href=\"#comparacao\">Comparação por Dataset</a>')\n",
    "html_lines.append('</div>')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0b8b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_lines.append('<h2 id=\"modelo1\">Modelo 1: Logistic Regression (Baseline) <a href=\"#inicio\"> Voltar ao topo</a></h2>')\n",
    "html_lines.append(\"<h3>Médias das Métricas</h3>\")\n",
    "html_lines.append(\"<ul>\")\n",
    "html_lines.append(f\"<li>Accuracy: {avg_metrics['Accuracy']:.3f}</li>\")\n",
    "html_lines.append(f\"<li>Precision: {avg_metrics['Precision']:.3f}</li>\")\n",
    "html_lines.append(f\"<li>Recall: {avg_metrics['Recall']:.3f}</li>\")\n",
    "html_lines.append(f\"<li>F1-Score: {avg_metrics['F1-Score']:.3f}</li>\")\n",
    "html_lines.append(f\"<li>ROC AUC: {avg_metrics['ROC AUC']:.3f}</li>\")\n",
    "html_lines.append(\"</ul>\")\n",
    "\n",
    "html_lines.append('<table id=\"table_baseline\"><tr>')\n",
    "cols = ['Dataset', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC AUC']\n",
    "for i, col in enumerate(cols):\n",
    "    html_lines.append(f'<th onclick=\"sortTable({i}, \\'table_baseline\\')\">{col}</th>')\n",
    "html_lines.append(\"</tr>\")\n",
    "for _, row in metrics_df.iterrows():\n",
    "    html_lines.append(\"<tr>\")\n",
    "    for col in cols:\n",
    "        val = row[col]\n",
    "        html_lines.append(f\"<td>{val:.3f}</td>\" if isinstance(val, float) else f\"<td>{val}</td>\")\n",
    "    html_lines.append(\"</tr>\")\n",
    "html_lines.append(\"</table>\")\n",
    "\n",
    "html_lines.append(\"<h3>Curvas ROC</h3><div style='display: flex; flex-wrap: wrap;'>\")\n",
    "for name, img_b64, auc in roc_images:\n",
    "    html_lines.append(\"<div style='margin:10px; text-align:center;'>\")\n",
    "    html_lines.append(f\"<img src='data:image/png;base64,{img_b64}' width='300'><br>\")\n",
    "    html_lines.append(f\"<span><b>{name}</b><br>AUC = {auc:.3f}</span>\")\n",
    "    html_lines.append(\"</div>\")\n",
    "html_lines.append(\"</div><hr>\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07632f2a",
   "metadata": {},
   "source": [
    "We are going to make a HTML report to evaluate our metrics!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b94de0",
   "metadata": {},
   "source": [
    "The overall averages using the Logistic Regression with no modification were:\n",
    "\n",
    "- Accuracy: 0.639\n",
    "\n",
    "- Precision: 0.167\n",
    "\n",
    "- Recall: 0.507\n",
    "\n",
    "- F1-Score: 0.226\n",
    "\n",
    "- ROC AUC: 0.579"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3760c0de",
   "metadata": {},
   "source": [
    "And these results align what we were expecting because the **precision is low** due to the struggle to identify the minority class; **recall is higher** because it's correctly flagging some positives but with many false positives; **F1-score is approximately 0.22** what's in line with a precision-recall trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a0be22",
   "metadata": {},
   "source": [
    "Also, we have some datasets where the precision and the recall were 0 because the model predicted only one class due to the strong imbalance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf342dcb",
   "metadata": {},
   "source": [
    "And we have too somes cases with high accuracy and bad recall, a typical illusion of accuracy when the majority class dominates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831ea052",
   "metadata": {},
   "source": [
    "Now we can proceed for the next step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f16e267",
   "metadata": {},
   "source": [
    "# 6th Step - Model Training & Evaluation: Post‑Changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb7813c",
   "metadata": {},
   "source": [
    "For this next step we are going to change our initial algorithm by creating a new hiperparameter 'imbalance_penalty' that does:\n",
    "- Helps address class imbalance by **increasing the weight of the minority class** in the loss function.\n",
    "- If set to *>1.0* , the model will **penalize errors on the minority class more heavily**, making it more sensitive to that class.\n",
    "- Applied dynamically based on which class is underrepresented in the training data.\n",
    "- Affects both **loss calculation** and **gradient updates**, guiding the model to perform better on imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65fe19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossLogisticRegression:\n",
    "    def __init__(self, lr=0.01, penalty=None, C=0.01, tolerance=1e-4, max_iters=1000, imbalance_penalty=1.0):\n",
    "        self.lr = lr\n",
    "        self.penalty = penalty    # 'l1', 'l2', or None\n",
    "        self.C = C\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iters = max_iters\n",
    "        self.imbalance_penalty = imbalance_penalty\n",
    "        self.coef_ = None  # Includes intercept as first element\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        N = len(y)\n",
    "        # Determine class counts\n",
    "        N_pos = np.sum(y == 1)\n",
    "        N_neg = np.sum(y == 0)\n",
    "        # Setup class weights manually based on imbalance_penalty\n",
    "        if N_pos == 0 or N_neg == 0:\n",
    "            # If one class is absent, no weighting needed\n",
    "            self.w_pos = 1.0\n",
    "            self.w_neg = 1.0\n",
    "        else:\n",
    "            if self.imbalance_penalty != 1.0:\n",
    "                # Identify minority class\n",
    "                if N_pos < N_neg:\n",
    "                    self.w_pos = self.imbalance_penalty\n",
    "                    self.w_neg = 1.0\n",
    "                elif N_neg < N_pos:\n",
    "                    self.w_pos = 1.0\n",
    "                    self.w_neg = self.imbalance_penalty\n",
    "                else:\n",
    "                    # If classes are balanced, no extra weighting\n",
    "                    self.w_pos = 1.0\n",
    "                    self.w_neg = 1.0\n",
    "            else:\n",
    "                # No imbalance penalty => no weighting\n",
    "                self.w_pos = 1.0\n",
    "                self.w_neg = 1.0\n",
    "\n",
    "        # Add intercept term\n",
    "        X_b = np.hstack([np.ones((N,1)), X])\n",
    "        # Initialize coefficients (bias + weights)\n",
    "        self.coef_ = np.zeros(X_b.shape[1])\n",
    "\n",
    "        prev_cost = float('inf')\n",
    "        for i in range(self.max_iters):\n",
    "            # Compute predictions\n",
    "            z = X_b.dot(self.coef_)\n",
    "            y_pred = self.sigmoid(z)\n",
    "            # Clip predictions to avoid log(0)\n",
    "            eps = 1e-15\n",
    "            y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "\n",
    "            # Compute weighted binary cross-entropy loss\n",
    "            cost = -(1.0 / N) * (\n",
    "                self.w_pos * (y * np.log(y_pred)).sum() +\n",
    "                self.w_neg * ((1 - y) * np.log(1 - y_pred)).sum()\n",
    "            )\n",
    "            # Add regularization penalty (skip intercept at index 0)\n",
    "            if self.penalty == 'l2':\n",
    "                cost += 0.5 * self.C * np.sum(self.coef_[1:]**2)\n",
    "            elif self.penalty == 'l1':\n",
    "                cost += self.C * np.sum(np.abs(self.coef_[1:]))\n",
    "\n",
    "            # Check for convergence\n",
    "            if abs(prev_cost - cost) < self.tolerance:\n",
    "                break\n",
    "            prev_cost = cost\n",
    "\n",
    "            # Compute gradient of weighted loss\n",
    "            error = y_pred - y\n",
    "            weights = np.where(y == 1, self.w_pos, self.w_neg)\n",
    "            gradient = (X_b.T.dot(weights * error)) / N\n",
    "\n",
    "            # Add gradient of penalty (not applying to intercept)\n",
    "            if self.penalty == 'l2':\n",
    "                gradient[1:] += self.C * self.coef_[1:]\n",
    "            elif self.penalty == 'l1':\n",
    "                gradient[1:] += self.C * np.sign(self.coef_[1:])\n",
    "\n",
    "            # Gradient descent update\n",
    "            self.coef_ -= self.lr * gradient\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.array(X)\n",
    "        N = X.shape[0]\n",
    "        X_b = np.hstack([np.ones((N,1)), X])\n",
    "        return self.sigmoid(X_b.dot(self.coef_))\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb39416",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data and Initialize Structures\n",
    "\n",
    "data_folder = \"class_imbalance_final\"\n",
    "csv_files = sorted(glob.glob(os.path.join(data_folder, \"*.csv\")))\n",
    "\n",
    "# Lists to accumulate results\n",
    "results1 = []  # Will hold dicts of metrics for each dataset\n",
    "roc_images1 = []  # Will hold (dataset_name, base64_png, auc) for plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14742915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process Each Dataset\n",
    "# For each CSV:\n",
    "# 1. Read data into X (features) and y (binary target).\n",
    "# 2. Repeatedly split (stratified) until both classes present in train and test.\n",
    "# 3. Scale features with StandardScaler (fit on train, transform both).\n",
    "# 4. Train logistic regression.\n",
    "# 5. Compute metrics on test set.\n",
    "# 6. Plot and save ROC curve for this dataset.\n",
    "\n",
    "for file_path in tqdm(csv_files, desc=\"Datasets\"):\n",
    "    dataset_name = os.path.basename(file_path)\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Assume the last column is 'target' as preprocessed (0/1)\n",
    "    if 'target' not in df.columns:\n",
    "        # If not labeled, rename last col to 'target'\n",
    "        df = df.rename(columns={df.columns[-1]: 'target'})\n",
    "    X = df.drop(columns=['target']).values\n",
    "    y = df['target'].values\n",
    "\n",
    "    # Stratified split with retries to ensure both classes in train and test\n",
    "    seed = 0\n",
    "    while True:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.3, stratify=y, random_state=seed\n",
    "        )\n",
    "        # Check class presence\n",
    "        if len(np.unique(y_train)) == 2 and len(np.unique(y_test)) == 2:\n",
    "            break\n",
    "        seed += 1\n",
    "\n",
    "    # Feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Model training\n",
    "    model = LossLogisticRegression()  # using default parameters\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predictions and probabilities\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    try:\n",
    "        y_proba = model.predict_proba(X_test_scaled)[:, 1]  # probability of class 1\n",
    "    except:\n",
    "        # Some implementations might use a different method name\n",
    "        y_proba = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Compute metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Store results\n",
    "    results1.append({\n",
    "        'Dataset': dataset_name,\n",
    "        'Accuracy': acc,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1-Score': f1,\n",
    "        'ROC AUC': auc\n",
    "    })\n",
    "\n",
    "    # Plot ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {auc:.3f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "    plt.title(f'ROC Curve: {dataset_name}')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot to base64\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    plt.close()\n",
    "    buf.seek(0)\n",
    "    img_base64 = base64.b64encode(buf.read()).decode('utf-8')\n",
    "    roc_images1.append((dataset_name, img_base64, auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cd6a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile Metrics and Compute Averages\n",
    "\n",
    "metrics_df1 = pd.DataFrame(results1)\n",
    "# Round metrics for display\n",
    "metrics_df1[['Accuracy','Precision','Recall','F1-Score','ROC AUC']] = metrics_df1[\n",
    "    ['Accuracy','Precision','Recall','F1-Score','ROC AUC']].round(3)\n",
    "\n",
    "# Compute average metrics\n",
    "avg_metrics1 = metrics_df1[['Accuracy','Precision','Recall','F1-Score','ROC AUC']].mean().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de1c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_lines.append('<h2 id=\"modelo2\">Modelo 2: Logistic Regression (After Changes) <a href=\"#inicio\">🔝 Voltar ao topo</a></h2>')\n",
    "html_lines.append(\"<h3>Médias das Métricas</h3>\")\n",
    "html_lines.append(\"<ul>\")\n",
    "html_lines.append(f\"<li>Accuracy: {avg_metrics1['Accuracy']:.3f}</li>\")\n",
    "html_lines.append(f\"<li>Precision: {avg_metrics1['Precision']:.3f}</li>\")\n",
    "html_lines.append(f\"<li>Recall: {avg_metrics1['Recall']:.3f}</li>\")\n",
    "html_lines.append(f\"<li>F1-Score: {avg_metrics1['F1-Score']:.3f}</li>\")\n",
    "html_lines.append(f\"<li>ROC AUC: {avg_metrics1['ROC AUC']:.3f}</li>\")\n",
    "html_lines.append(\"</ul>\")\n",
    "\n",
    "html_lines.append('<table id=\"table_after\"><tr>')\n",
    "for i, col in enumerate(cols):\n",
    "    html_lines.append(f'<th onclick=\"sortTable({i}, \\'table_after\\')\">{col}</th>')\n",
    "html_lines.append(\"</tr>\")\n",
    "for _, row in metrics_df1.iterrows():\n",
    "    html_lines.append(\"<tr>\")\n",
    "    for col in cols:\n",
    "        val = row[col]\n",
    "        html_lines.append(f\"<td>{val:.3f}</td>\" if isinstance(val, float) else f\"<td>{val}</td>\")\n",
    "    html_lines.append(\"</tr>\")\n",
    "html_lines.append(\"</table>\")\n",
    "\n",
    "html_lines.append(\"<h3>Curvas ROC</h3><div style='display: flex; flex-wrap: wrap;'>\")\n",
    "for name, img_b64, auc in roc_images1:\n",
    "    html_lines.append(\"<div style='margin:10px; text-align:center;'>\")\n",
    "    html_lines.append(f\"<img src='data:image/png;base64,{img_b64}' width='300'><br>\")\n",
    "    html_lines.append(f\"<span><b>{name}</b><br>AUC = {auc:.3f}</span>\")\n",
    "    html_lines.append(\"</div>\")\n",
    "html_lines.append(\"</div><hr>\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f0cc38",
   "metadata": {},
   "source": [
    "Now we are also going to compare the result using our changed algorithm with SMOTE (Synthetic Minority Over-sampling Technique) to see if the results change!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923aee1c",
   "metadata": {},
   "source": [
    "We are using k=4 neighbours instead of the default k=5 because SMOTE requires at least *k+1* minority class samples, and we have some cases with only 5 minority cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4493da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data and Initialize Structures\n",
    "data_folder = \"class_imbalance_final\"\n",
    "csv_files = sorted(glob.glob(os.path.join(data_folder, \"*.csv\")))\n",
    "\n",
    "results2 = []  # Will hold dicts of metrics for each dataset\n",
    "roc_images2 = []  # Will hold (dataset_name, base64_png, auc) for plots\n",
    "\n",
    "# Process each dataset\n",
    "for file_path in tqdm(csv_files, desc=\"Datasets\"):\n",
    "    dataset_name = os.path.basename(file_path)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Assume the last column is 'target' as preprocessed (0/1)\n",
    "    if 'target' not in df.columns:\n",
    "        df = df.rename(columns={df.columns[-1]: 'target'})\n",
    "\n",
    "    X = df.drop(columns=['target']).values\n",
    "    y = df['target'].values\n",
    "\n",
    "    # Stratified split with retries to ensure both classes in train and test\n",
    "    seed = 0\n",
    "    while True:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.3, stratify=y, random_state=seed\n",
    "        )\n",
    "        if len(np.unique(y_train)) == 2 and len(np.unique(y_test)) == 2:\n",
    "            break\n",
    "        seed += 1\n",
    "\n",
    "    # Feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Apply SMOTE to training data\n",
    "    smote = SMOTE(random_state=42, k_neighbors=4)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "    # Model training\n",
    "    model = LossLogisticRegression()\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Predictions and probabilities\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    try:\n",
    "        y_proba = model.predict_proba(X_test_scaled)\n",
    "    except:\n",
    "        y_proba = y_pred  # fallback if proba isn't supported\n",
    "\n",
    "    # Compute metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Store results\n",
    "    results2.append({\n",
    "        'Dataset': dataset_name,\n",
    "        'Accuracy': acc,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1-Score': f1,\n",
    "        'ROC AUC': auc\n",
    "    })\n",
    "\n",
    "    # Plot ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {auc:.3f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "    plt.title(f'ROC Curve: {dataset_name}')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot to base64\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    plt.close()\n",
    "    buf.seek(0)\n",
    "    img_base64 = base64.b64encode(buf.read()).decode('utf-8')\n",
    "    roc_images2.append((dataset_name, img_base64, auc))\n",
    "\n",
    "    # Convert results list to DataFrame\n",
    "metrics_df2 = pd.DataFrame(results2)\n",
    "\n",
    "# Compute average metrics\n",
    "avg_metrics2= metrics_df2[['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC AUC']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46524795",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_lines.append('<h2 id=\"modelo3\">Modelo 3: Logistic Regression (With SMOTE) <a href=\"#inicio\">🔝 Voltar ao topo</a></h2>')\n",
    "html_lines.append(\"<h3>Médias das Métricas</h3>\")\n",
    "html_lines.append(\"<ul>\")\n",
    "html_lines.append(f\"<li>Accuracy: {avg_metrics2['Accuracy']:.3f}</li>\")\n",
    "html_lines.append(f\"<li>Precision: {avg_metrics2['Precision']:.3f}</li>\")\n",
    "html_lines.append(f\"<li>Recall: {avg_metrics2['Recall']:.3f}</li>\")\n",
    "html_lines.append(f\"<li>F1-Score: {avg_metrics2['F1-Score']:.3f}</li>\")\n",
    "html_lines.append(f\"<li>ROC AUC: {avg_metrics2['ROC AUC']:.3f}</li>\")\n",
    "html_lines.append(\"</ul>\")\n",
    "\n",
    "html_lines.append('<table id=\"table_smote\"><tr>')\n",
    "for i, col in enumerate(cols):\n",
    "    html_lines.append(f'<th onclick=\"sortTable({i}, \\'table_smote\\')\">{col}</th>')\n",
    "html_lines.append(\"</tr>\")\n",
    "for _, row in metrics_df2.iterrows():\n",
    "    html_lines.append(\"<tr>\")\n",
    "    for col in cols:\n",
    "        val = row[col]\n",
    "        html_lines.append(f\"<td>{val:.3f}</td>\" if isinstance(val, float) else f\"<td>{val}</td>\")\n",
    "    html_lines.append(\"</tr>\")\n",
    "html_lines.append(\"</table>\")\n",
    "\n",
    "html_lines.append(\"<h3>Curvas ROC</h3><div style='display: flex; flex-wrap: wrap;'>\")\n",
    "for name, img_b64, auc in roc_images2:\n",
    "    html_lines.append(\"<div style='margin:10px; text-align:center;'>\")\n",
    "    html_lines.append(f\"<img src='data:image/png;base64,{img_b64}' width='300'><br>\")\n",
    "    html_lines.append(f\"<span><b>{name}</b><br>AUC = {auc:.3f}</span>\")\n",
    "    html_lines.append(\"</div>\")\n",
    "html_lines.append(\"</div><hr>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f69ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_lines.append('<hr>')\n",
    "html_lines.append('<a id=\"comparacao\"></a>')\n",
    "html_lines.append('<h2>🔍 Comparação por Dataset</h2>')\n",
    "html_lines.append('<div style=\"margin-top: 20px;\"><a href=\"#inicio\">🔝 Voltar ao topo</a></div>')\n",
    "html_lines.append(\"\"\"\n",
    "<label for=\"datasetSelect\">Escolhe um dataset:</label>\n",
    "<select id=\"datasetSelect\" onchange=\"compararDataset()\">\n",
    "  <option value=\"\">-- Seleciona --</option>\n",
    "</select>\n",
    "\n",
    "<table id=\"comparacaoResultados\" style=\"margin-top:15px; display:none;\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Modelo</th>\n",
    "      <th>Accuracy</th>\n",
    "      <th>Precision</th>\n",
    "      <th>Recall</th>\n",
    "      <th>F1-Score</th>\n",
    "      <th>ROC AUC</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody></tbody>\n",
    "</table>\n",
    "\n",
    "<script>\n",
    "const modelos = [\"Baseline\", \"After Changes\", \"With SMOTE\"];\n",
    "const tabelas = {\n",
    "  \"Baseline\": document.getElementById(\"table_baseline\"),\n",
    "  \"After Changes\": document.getElementById(\"table_after\"),\n",
    "  \"With SMOTE\": document.getElementById(\"table_smote\")\n",
    "};\n",
    "\n",
    "\n",
    "// Recolher todos os nomes de dataset existentes\n",
    "const nomesDatasets = new Set();\n",
    "for (const modelo in tabelas) {\n",
    "  const linhas = tabelas[modelo].querySelectorAll(\"tbody tr\");\n",
    "  for (const linha of linhas) {\n",
    "    const nome = linha.children[0].textContent.trim();\n",
    "    nomesDatasets.add(nome);\n",
    "  }\n",
    "}\n",
    "\n",
    "// Preencher o dropdown\n",
    "const select = document.getElementById(\"datasetSelect\");\n",
    "[...nomesDatasets].sort().forEach(nome => {\n",
    "  const option = document.createElement(\"option\");\n",
    "  option.value = nome;\n",
    "  option.textContent = nome;\n",
    "  select.appendChild(option);\n",
    "});\n",
    "\n",
    "function compararDataset() {\n",
    "  const ds = select.value;\n",
    "  const tabela = document.getElementById(\"comparacaoResultados\");\n",
    "  const tbody = tabela.querySelector(\"tbody\");\n",
    "  tbody.innerHTML = \"\";\n",
    "\n",
    "  if (!ds) {\n",
    "    tabela.style.display = \"none\";\n",
    "    return;\n",
    "  }\n",
    "\n",
    "  for (const modelo in tabelas) {\n",
    "    const linhas = tabelas[modelo].querySelectorAll(\"tbody tr\");\n",
    "    for (const linha of linhas) {\n",
    "      if (linha.children[0].textContent.trim() === ds) {\n",
    "        const novaLinha = document.createElement(\"tr\");\n",
    "        novaLinha.innerHTML = `<td>${modelo}</td>` +\n",
    "          [...linha.children].slice(1).map(td => `<td>${td.textContent}</td>`).join(\"\");\n",
    "        tbody.appendChild(novaLinha);\n",
    "        break;\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  tabela.style.display = \"table\";\n",
    "}\n",
    "</script>\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9fffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_lines.append(\"</body></html>\")\n",
    "\n",
    "with open(\"relatorio_comparativo_final.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(html_lines))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c4e3ab",
   "metadata": {},
   "source": [
    "We observed that adding SMOTE to a cost-sensitive logistic regression led to modest improvements, particularly in AUC but overall the gains were incremental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42353d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_lines = []\n",
    "\n",
    "# Cabeçalho com CSS e JavaScript\n",
    "html_lines.append(\"<html><head><title>Relatório Comparativo</title>\")\n",
    "html_lines.append(\"<style>\")\n",
    "html_lines.append(\"body {font-family: Arial, sans-serif; margin: 20px; background-color: #f0f8ff; color: #222;}\")\n",
    "html_lines.append(\"h1, h2, h3 { color: #007acc; }\")\n",
    "html_lines.append(\"table {border-collapse: collapse; width: 100%; margin-bottom: 30px; background-color: #ffffff;}\")\n",
    "html_lines.append(\"th, td {border: 1px solid #ddd; padding: 8px; text-align: center;}\")\n",
    "html_lines.append(\"th {background-color: #e6f2ff; cursor: pointer;}\")\n",
    "html_lines.append(\"th:hover {background-color: #d0e7ff;}\")\n",
    "html_lines.append(\"tr:nth-child(even) {background-color: #f9fbfd;}\")\n",
    "html_lines.append(\"tr:hover {background-color: #eef7ff;}\")\n",
    "html_lines.append(\"img {border: 1px solid #ccc; margin: 5px;}\")\n",
    "html_lines.append(\"a { color: #007acc; text-decoration: none; }\")\n",
    "html_lines.append(\"a:hover { text-decoration: underline; }\")\n",
    "html_lines.append(\"button { background-color: #007acc; color: white; border: none; padding: 8px 12px; border-radius: 4px; cursor: pointer; }\")\n",
    "html_lines.append(\"button:hover { background-color: #005f99; }\")\n",
    "html_lines.append(\"</style>\")\n",
    "html_lines.append(\"\"\"\n",
    "<script>\n",
    "function sortTable(n, id) {\n",
    "  var table = document.getElementById(id), rows, switching = true, i, x, y, shouldSwitch, dir = \"asc\", switchcount = 0;\n",
    "  while (switching) {\n",
    "    switching = false;\n",
    "    rows = table.rows;\n",
    "    for (i = 1; i < (rows.length - 1); i++) {\n",
    "      shouldSwitch = false;\n",
    "      x = rows[i].getElementsByTagName(\"TD\")[n];\n",
    "      y = rows[i + 1].getElementsByTagName(\"TD\")[n];\n",
    "      var xVal = isNaN(x.innerHTML) ? x.innerHTML.toLowerCase() : parseFloat(x.innerHTML);\n",
    "      var yVal = isNaN(y.innerHTML) ? y.innerHTML.toLowerCase() : parseFloat(y.innerHTML);\n",
    "      if ((dir === \"asc\" && xVal > yVal) || (dir === \"desc\" && xVal < yVal)) {\n",
    "        shouldSwitch = true; break;\n",
    "      }\n",
    "    }\n",
    "    if (shouldSwitch) {\n",
    "      rows[i].parentNode.insertBefore(rows[i + 1], rows[i]);\n",
    "      switching = true; switchcount++;\n",
    "    } else {\n",
    "      if (switchcount === 0 && dir === \"asc\") {\n",
    "        dir = \"desc\"; switching = true;\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "</script>\n",
    "\"\"\")\n",
    "html_lines.append(\"</head><body>\")\n",
    "html_lines.append(\"<h1>Relatório Comparativo: Modelos de Classificação</h1>\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e745891",
   "metadata": {},
   "source": [
    "# 7th Step - Creating the HTML for results viewing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241c022a",
   "metadata": {},
   "source": [
    "Now that we have all the results we are going to create two HTML files for comparison of results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916c82e2",
   "metadata": {},
   "source": [
    "We are going to be able to compare the results through models and through datasets as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dce2c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume these DataFrames exist from earlier steps:\n",
    "# metrics_df  -> Baseline metrics (indexed by 'Dataset')\n",
    "# metrics_df1 -> After Changes metrics (indexed by 'Dataset')\n",
    "# metrics_df2 -> Changes + SMOTE metrics (indexed by 'Dataset')\n",
    "\n",
    "# List of metric names (as in the notebook)\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC AUC']\n",
    "# List of models and colors\n",
    "models = ['Baseline', 'After Changes', 'Changes + SMOTE']\n",
    "colors = {'Baseline': 'steelblue', \n",
    "          'After Changes': 'orange', \n",
    "          'Changes + SMOTE': 'green'}\n",
    "\n",
    "# Ensure 'Dataset' is index (if not already)\n",
    "metrics_df = metrics_df.set_index('Dataset')\n",
    "metrics_df1 = metrics_df1.set_index('Dataset')\n",
    "metrics_df2 = metrics_df2.set_index('Dataset')\n",
    "\n",
    "# All dataset names to include in dropdown\n",
    "dataset_names = list(metrics_df.index)\n",
    "\n",
    "# Create subplots: 1 row x 2 cols\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('Metrics Comparison', 'ROC Curve'))\n",
    "\n",
    "# Initial dataset (first one)\n",
    "initial_ds = dataset_names[0]\n",
    "# Extract initial y-values for bars\n",
    "y_base = [metrics_df.loc[initial_ds, m]    for m in metrics]\n",
    "y_after = [metrics_df1.loc[initial_ds, m]  for m in metrics]\n",
    "y_smote = [metrics_df2.loc[initial_ds, m]  for m in metrics]\n",
    "\n",
    "# Add bar chart traces (one trace per model)\n",
    "fig.add_trace(go.Bar(name='Baseline', x=metrics, y=y_base, marker_color=colors['Baseline']), row=1, col=1)\n",
    "fig.add_trace(go.Bar(name='After Changes', x=metrics, y=y_after, marker_color=colors['After Changes']), row=1, col=1)\n",
    "fig.add_trace(go.Bar(name='Changes + SMOTE', x=metrics, y=y_smote, marker_color=colors['Changes + SMOTE']), row=1, col=1)\n",
    "\n",
    "# Add ROC curve traces for initial dataset (dummy example curves here)\n",
    "# In practice, replace x= and y= with the actual fpr/tpr arrays for each model.\n",
    "fig.add_trace(go.Scatter(x=[0,1], y=[0,1], mode='lines', name='Baseline ROC', \n",
    "                         line=dict(color=colors['Baseline'])), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=[0,1], y=[0.1,0.9], mode='lines', name='After Changes ROC', \n",
    "                         line=dict(color=colors['After Changes'])), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=[0,1], y=[0.05,0.95], mode='lines', name='Changes+SMOTE ROC', \n",
    "                         line=dict(color=colors['Changes + SMOTE'], dash='dash')), row=1, col=2)\n",
    "\n",
    "# Update layout titles\n",
    "fig.update_layout(title_text=f\"Model Comparison: Dataset = {initial_ds}\", \n",
    "                  showlegend=True, \n",
    "                  legend_title_text='Model',\n",
    "                  yaxis=dict(title='Score'), \n",
    "                  yaxis2=dict(title='True Positive Rate'),\n",
    "                  xaxis2=dict(title='False Positive Rate'))\n",
    "\n",
    "# Prepare dropdown buttons (one per dataset)\n",
    "buttons = []\n",
    "for ds in dataset_names:\n",
    "    # New bar heights for this dataset\n",
    "    yb = [metrics_df.loc[ds, m]    for m in metrics]\n",
    "    ya = [metrics_df1.loc[ds, m]   for m in metrics]\n",
    "    ys = [metrics_df2.loc[ds, m]   for m in metrics]\n",
    "    # (Here we would also compute the new ROC curves if we have data)\n",
    "    # For example, new ROC points could be precomputed fpr/ tpr arrays:\n",
    "    # fpr_base, tpr_base = compute_roc('Baseline', ds)\n",
    "    # etc. As placeholders, we'll keep the same line segments.\n",
    "    new_y = [yb, ya, ys,  # bar traces update\n",
    "             [0,1], [0.1,0.9], [0.05,0.95]]  # ROC traces (dummy here)\n",
    "    buttons.append(dict(label=ds,\n",
    "                        method='update',\n",
    "                        args=[{\n",
    "                            'y': new_y,\n",
    "                            'x': [metrics, metrics, metrics, [0,1], [0,1], [0,1]]\n",
    "                        }, {\n",
    "                            'title': f\"Model Comparison: Dataset = {ds}\"\n",
    "                        }]))\n",
    "# Add dropdown to layout\n",
    "fig.update_layout(\n",
    "    updatemenus=[dict(active=0, buttons=buttons, \n",
    "                      x=0.5, xanchor='center', y=1.15, yanchor='top')],\n",
    "    margin=dict(t=100)  # make space for dropdown\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad84f6c",
   "metadata": {},
   "source": [
    "# **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fb7964",
   "metadata": {},
   "source": [
    "| Metric    | Baseline | After Changes | Changes + SMOTE |\n",
    "| --------- | -------- | ------------- | --------------- |\n",
    "| Accuracy  | 0.639    | 0.935         | 0.785           |\n",
    "| Precision | 0.167    | 0.707         | 0.359           |\n",
    "| Recall    | 0.507    | 0.429         | 0.845           |\n",
    "| F1-Score  | 0.226    | 0.490         | 0.477           |\n",
    "| ROC AUC   | 0.579    | 0.709         | 0.881           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458bb3ba",
   "metadata": {},
   "source": [
    "The model changes led to significant improvements in accuracy, precision, and F1-Score, although recall slightly decreased. After applying SMOTE, recall increased substantially (to 0.845), and overall performance improved (ROC AUC of 0.881), despite a drop in precision. Therefore, the SMOTE-enhanced model is more suitable when identifying positive cases is critical, while the model without SMOTE is preferable when higher precision is required."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
